# 🎙️ Zoom Transcription + Q&A App (RAG-based)

This project allows users to upload Zoom meeting recordings, transcribe them, and then ask natural language questions about the meeting content using an LLM (like GPT-3.5) with Retrieval-Augmented Generation (RAG).

---

## 🚀 Features

- Upload audio/video recordings from Zoom
- Transcription using `faster-whisper`
- Store transcript in a vector DB (Chroma)
- Ask questions about the meeting
- Smart answers generated by OpenAI GPT models

---

## 🛠️ Tech Stack

- Python
- Gradio (UI)
- Faster-Whisper (speech-to-text)
- ChromaDB (vector database)
- LangChain (RAG pipeline)
- OpenAI API (embeddings + LLM)
- dotenv for environment config

---

## ⚙️ Setup Instructions

### 1. Clone this repo
```bash
git clone https://github.com/your-username/zoom-transcriber-rag.git
cd zoom-transcriber-rag
2. Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Create .env file from the template
bash
Copy
Edit
cp .env.example .env
Then open .env and set your OpenAI key.

4. Run the app
bash
Copy
Edit
python zoom_rag.py
A Gradio web UI will open in your browser.

🔐 Environment Variables
All secrets are stored in .env file (do NOT commit this to GitHub):

ini
Copy
Edit
OPENAI_API_KEY=your_openai_api_key_here
🙋‍♀️ How it works
You upload a Zoom audio/video file.

It’s transcribed using faster-whisper.

Transcript is split and stored in ChromaDB using OpenAI embeddings.

When you ask a question, relevant chunks are retrieved using similarity search.

The LLM (ChatGPT via API) answers your question using that context.

 Credits
Built by Nandini as a GenAI experiment.
Based on LangChain, OpenAI, ChromaDB, Faster-Whisper.

## 🧾 `.env.example`

> Save this as `.env.example`

```env
# Rename this file to `.env` and add your actual OpenAI key below
OPENAI_API_KEY=your_openai_api_key_here
